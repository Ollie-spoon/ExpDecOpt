{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains optimization code to fit a series of multi-exponential decays to patient data our NMR scans of the patient's skeletal muscles. It is designed so that you should be able to run every block in succession after the initial function imports so that functions don't need to be recompiled.\n",
    "\n",
    "The core ideas in the optimization process used are that newton/quasi-newton optimisation methods work really well for exponential decay fitting due to their ability to cope with wide trough-like loss landscapes. e.g.\n",
    "\n",
    "    f(x, y) = x^2 + y^2/1000\n",
    "\n",
    "However, there is an issue here because newton methods are local rather than global, so we're using a sampling method to sample starting points. To do this, we start with one decay, where the loss landscape typically only has a single minimum and then use the results of the mono-exponential decay fitting to produce a number of candidate starting points for two decays. We then repeat this process for three decays using the information from the bi-exponential best fit.\n",
    "\n",
    "The specific optimization algorithm used is BFGS - a quasi-newton method. In theory it is not as effective as the pure newton method in certain circumstances, however, it is more robust to starting conditions so we're using it for the moment.\n",
    "\n",
    "The essence of this local sampling method is that by sampling enough points to perform local optimziations from, we should always be able to find a global minimum. I will also note here that due to the physical nature of our measurements we know that all amplitudes and decay constants must be positive by definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all the necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from jax import random, jit\n",
    "from jax import numpy as jnp\n",
    "from jax.numpy.fft import fft, ifft\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from jaxopt import BFGS\n",
    "jnp_float = jnp.float32\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from contextlib import contextmanager\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from scipy.stats import shapiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The use_adjustment parameter was part of a test that we were running to \n",
    "## try and account for issues in our data processing\n",
    "\n",
    "# use_adjustment indicates whether the parameter-less loss adjustment should be used\n",
    "use_adjustment = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function definitions\n",
    "\n",
    "# function to allow print suppression because the BFGS solver in jaxopt returns warnings as print statements\n",
    "@contextmanager\n",
    "def suppress_print():\n",
    "    # Save the current stdout\n",
    "    original_stdout = sys.stdout\n",
    "    # Redirect stdout to null\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        # Restore the original stdout\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = original_stdout\n",
    "\n",
    "# exponential decay function\n",
    "@jit\n",
    "def exponential(t, A1, tau1):\n",
    "    return A1 * jnp.exp(-t / tau1)\n",
    "\n",
    "def multi_exponential(t, *params):\n",
    "    x, offset = params[:-1], params[-1]\n",
    "    n = len(x) // 2\n",
    "    return jnp.sum(jnp.array([exponential(t, x[2*i], x[2*i+1]) for i in range(n)], dtype=jnp_float), axis=0) - offset\n",
    "\n",
    "# multi-exponential decay function containing the offset if specified\n",
    "@jit\n",
    "def fit_multi_exponential(t, *params):\n",
    "    \n",
    "    x = params\n",
    "    \n",
    "    n = len(x) // 2\n",
    "    \n",
    "    decay = jnp.sum(jnp.array([exponential(t, x[2*i], x[2*i+1]) for i in range(n)], dtype=jnp_float), axis=0)\n",
    "    \n",
    "    last_mean_offset = jnp.mean(decay[-1000:-10]) if use_adjustment else 0\n",
    "    \n",
    "    return decay - last_mean_offset\n",
    "\n",
    "# printout the parameters for a decay\n",
    "def print_params(params):\n",
    "    x = params\n",
    "    for i in range(0, len(x), 2):\n",
    "        print(f\"decay {i//2 + 1}: {x[i]:.4f} {x[i+1]:.2f}\")\n",
    "\n",
    "# durbin-watson autocorrelation test\n",
    "def durbin_watson(residuals): \n",
    "    \"\"\"\n",
    "    This is a modified version of the Durbin-Watson statistic that is \n",
    "    used to test for autocorrelation in the residuals.\n",
    "    \n",
    "    The output has been modified to go from 0 to 1 instead of 0 to 4 to match \n",
    "    the range of theshapiro-wilk p-value.\n",
    "    \n",
    "    0 indicates no correlation and 1 indicates perfect correlation.\n",
    "    \"\"\"\n",
    "    diff = jnp.diff(residuals) \n",
    "    dw_stat = jnp.sum(diff ** 2) / jnp.sum(residuals ** 2) \n",
    "    return jnp.abs(dw_stat-2)/2.0\n",
    "\n",
    "# shapiro-wilk test for normality\n",
    "def shapiro_wilk(residuals):\n",
    "    \"\"\"\n",
    "    This is the Shapiro-Wilk test for normality.\n",
    "    \n",
    "    The output is the test statistic and the p-value.\n",
    "    The statistic has a range of 0 to 1, but will mostly be between 0.9 and 1.\n",
    "    1 indicates normality, but the p-value is the most important output.\n",
    "    The p-value represents the probability that the data is normally distributed.\n",
    "    A common threshold for normality is 0.05, above which the data is considered normal.\n",
    "    \"\"\"\n",
    "    return shapiro(residuals)\n",
    "\n",
    "# produces a solver function to optimize the parameters of a decay\n",
    "def get_solve(maxiter, atol, time_data, noisy_signal, magnitudes):\n",
    "    \"\"\"\n",
    "    The core idea is to jit compile anything that is going to run many times.\n",
    "    \n",
    "    It works out faster to optimize for 2000 steps with jit optimization than \n",
    "    to set a tolerance and let the solver run until it converges, especially \n",
    "    when we're performing many optimizations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # defines functions to dedimensionalize and normalize the parameters to \n",
    "    # help with ill-conditioned inputs\n",
    "    forward = jit(lambda x: x / magnitudes)\n",
    "    backward = jit(lambda x: x * magnitudes)\n",
    "    \n",
    "    # loss function takes values in dedimensionalized form and returns the loss\n",
    "    @jit\n",
    "    def loss_fn(params):\n",
    "        params = backward(params)\n",
    "        return jnp.sum((fit_multi_exponential(time_data, *params) - noisy_signal) ** 2)\n",
    "        \n",
    "    # Define the solver\n",
    "    solver = BFGS(\n",
    "        fun=loss_fn,\n",
    "        maxiter=2000,\n",
    "        tol=1e-20,\n",
    "        max_stepsize=0.5,\n",
    "        min_stepsize=1e-20,\n",
    "    )\n",
    "    \n",
    "    @jit\n",
    "    def init_state(params):\n",
    "        return solver.init_state(params)\n",
    "    \n",
    "    @jit\n",
    "    def update(params, state):\n",
    "        return solver.update(params, state)\n",
    "    \n",
    "    @jit \n",
    "    def is_close(x, y):\n",
    "        return jnp.isclose(x, y, atol=atol)\n",
    "    \n",
    "    # For loops take forever to compile, it's much quicker and easier to jit compile a \n",
    "    # single update step and loop over that\n",
    "    def solve(params):\n",
    "        params = forward(params)\n",
    "        state = init_state(params)\n",
    "        \n",
    "        last = -1\n",
    "        same_last_count = 0\n",
    "        with suppress_print():\n",
    "            for _ in range(maxiter):\n",
    "                params, state = update(params, state)\n",
    "                if is_close(state.value, last):\n",
    "                    same_last_count += 1\n",
    "                else:\n",
    "                    same_last_count = 0\n",
    "                if same_last_count > 10:\n",
    "                    break\n",
    "                last = state.value\n",
    "        \n",
    "        return backward(params), state\n",
    "    \n",
    "    return solve\n",
    "\n",
    "# produces a bootstrap solver function to produce a distribution of parameter \n",
    "# sets based on the noisy signal. \n",
    "# This is used to produce confidence intervals for the parameters. \n",
    "def get_bootstrap_solve(atol, time_data, noisy_signal, params):\n",
    "    \"\"\"\n",
    "    The bootstrapping process takes the parameters identified with fitting and uses them \n",
    "    as the starting point for an optimziation on a randomly sampled subset of the data. \n",
    "    \n",
    "    The core assumption here is that the identified parameters for the full signal are \n",
    "    local to the global minimum of the data subset. This should be valid in most cases, but \n",
    "    it is not guaranteed, especially when the noise has long tails. \n",
    "    \n",
    "    Bootstrapping is used here to produce a distribution of parameter sets that doesn't \n",
    "    rely on the assumption of normality. This is particularly important when assessing \n",
    "    different fitting methods where the resulting distributions\n",
    "    \n",
    "    Frankly, this was a pain to implement in jax, and it's not very pretty, but it does the job.\n",
    "    \"\"\"\n",
    "    \n",
    "    # defines functions to dedimensionalize and normalize the parameters\n",
    "    # for a smoother optimization\n",
    "    backward = jit(lambda x: x * params)\n",
    "    # forward is not needed here as params â‰¡ magnitudes so instead we have \n",
    "    params_i = jnp.ones_like(params)\n",
    "    \n",
    "    n = len(noisy_signal)\n",
    "    \n",
    "    @jit\n",
    "    def is_close(x, y):\n",
    "        return jnp.isclose(x, y, atol=atol)\n",
    "    \n",
    "    @jit\n",
    "    def initialize_bootstrap(key):\n",
    "        # sample bootstrap data indices from the noisy signal\n",
    "        bootstrap_indices = random.choice(key, n, (n,), replace=True)\n",
    "        noisy_signal_bootstrap = noisy_signal[bootstrap_indices]\n",
    "        time_data_bootstrap = time_data[bootstrap_indices]\n",
    "        \n",
    "        def loss_fn(params):\n",
    "            # transform the params to the correct scale\n",
    "            params = backward(params)\n",
    "            return jnp.sum((fit_multi_exponential(time_data_bootstrap, *params) - noisy_signal_bootstrap) ** 2)\n",
    "        \n",
    "        solver = BFGS(\n",
    "            fun=loss_fn,\n",
    "            maxiter=2000,\n",
    "            tol=1e-20,\n",
    "            max_stepsize=0.1,\n",
    "            min_stepsize=1e-20,\n",
    "        )\n",
    "        \n",
    "        state = solver.init_state(params_i)\n",
    "        \n",
    "        return state, noisy_signal_bootstrap, time_data_bootstrap\n",
    "    \n",
    "    # Solve 10 times in a row to exchange compile time for faster runtime\n",
    "    @jit\n",
    "    def solve_10(params, state, noisy_signal_bootstrap, time_data_bootstrap):\n",
    "        \n",
    "        def loss_fn(params):\n",
    "            # transform the params to the correct scale\n",
    "            params = backward(params)\n",
    "            return jnp.sum((fit_multi_exponential(time_data_bootstrap, *params) - noisy_signal_bootstrap) ** 2)\n",
    "        \n",
    "        solver = BFGS(\n",
    "            fun=loss_fn,\n",
    "            maxiter=2000,\n",
    "            tol=1e-20,\n",
    "            max_stepsize=0.1,\n",
    "            min_stepsize=1e-20,\n",
    "        )\n",
    "        \n",
    "        for _ in range(10):\n",
    "            params, state = solver.update(params, state)\n",
    "        \n",
    "        return params, state\n",
    "\n",
    "    # unlike the normal solver, the bootstrap solver does use an exit condition \n",
    "    def solve(key):\n",
    "        state, noisy_signal_bootstrap, time_data_bootstrap = initialize_bootstrap(key)\n",
    "        params = params_i\n",
    "        done = False\n",
    "        last = -1\n",
    "        before_last = -1\n",
    "        with suppress_print():\n",
    "            while not done:\n",
    "                params, state = solve_10(params, state, noisy_signal_bootstrap, time_data_bootstrap)\n",
    "                done = is_close(state.value, last) and is_close(before_last, last)\n",
    "                before_last = last\n",
    "                last = state.value\n",
    "                # quit if any of the params are nan or negative\n",
    "                if jnp.any(jnp.isnan(params)) or jnp.any(params < 0):\n",
    "                    break\n",
    "        return backward(params), state\n",
    "    \n",
    "    return solve\n",
    "\n",
    "# sort the parameters by the decay time\n",
    "def sort_params(params):\n",
    "    return params[params[:, 1].argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## controls used to reference specific files, if you want to run this code, I've attatched\n",
    "## some patient data in a separate file that you can trial it on but you might have to modify the import block below.\n",
    "# pre or post, with regard to patient data before and after dialysis\n",
    "post = 0 # 0 for pre, 1 for post\n",
    "prepost = \"Post\" if post else \"Pre\"\n",
    "\n",
    "\n",
    "# index of the patient number for file referencing\n",
    "index = 26\n",
    "\n",
    "# copy_dict is populated throughout the program to store information to be copied to excel\n",
    "copy_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#####################################\n",
    "#### IMPORT SIGNAL DATA FROM CSV ####\n",
    "#####################################\n",
    "\n",
    "def get_filename_from_index(index, prepost):\n",
    "    # Define the file paths\n",
    "    files = [\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\Post\\B_avgScans_HD05MGHI-Post.csv\", # 0\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\Post\\B_avgScans_HD08MGHI-Post.csv\", # 1\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\Post\\B_avgScans_HD06FKCC-Post.csv\", # 2\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\Post\\B_avgScans_HD07FKCC-Post.csv\", # 3\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\Post\\B_avgScans_HD08FKCC-Post.csv\", # 4\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\Post\\B_avgScans_HD09FKCC-Post.csv\", # 5\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\Post\\B_avgScans_HD03FKCC2-Post.csv\", # 6\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\Post\\B_avgScans_HD04FKCC2-Post.csv\", # 7\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\Post\\B_avgScans_HD07FKCC2-Post.csv\", # 8\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\Post\\B_avgScans_HD08FKCC2-Post.csv\", # 9\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\Post\\B_avgScans_HD02FKCS-Post.csv\", # 10\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\Post\\B_avgScans_HD03FKCS-Post.csv\", # 11\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\Post\\B_avgScans_HD04FKCS-Post.csv\", # 12\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\Post\\B_avgScans_HD05FKCS-Post.csv\", # 13\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\Post\\B_avgScans_HD06FKCS-Post.csv\", # 14\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\B_avgScans_HC01MGHRA-.csv\", # 15\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\B_avgScans_HC02MGHRA-.csv\", # 16\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\B_avgScans_HC03MGHRA-.csv\", # 17\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\B_avgScans_HC04MGHRA-.csv\", # 18\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\B_avgScans_HC05MGHRA-.csv\", # 19\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\B_avgScans_HC06MGHRA-.csv\", # 20\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\B_avgScans_HC07MGHRA-.csv\", # 21\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\B_avgScans_HC08MGHRA-.csv\", # 22\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\B_avgScans_HC09MGHRA-.csv\", # 23\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\B_avgScans_HC10MGHRA-.csv\", # 24\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\B_avgScans_HC11MGHRA-.csv\", # 25\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\B_avgScans_HC12MGHRA-.csv\", # 26\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\B_avgScans_HC13MGHRA-.csv\", # 27\n",
    "        r\"c:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\B_avgScans_HC14MGHRA-.csv\", # 28\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    # Check the index is valid\n",
    "    assert type(index) == int, \"index must be an integer\"\n",
    "    assert index < len(files), \"index must be less than the total number of files: \" + str(len(files))\n",
    "    assert index >= 0, \"index must be greater than or equal to 0\"\n",
    "    assert prepost.lower() in [\"pre\", \"post\"], \"prepost must be either 'pre' or 'post'\"\n",
    "    \n",
    "    # Get the filename and replace the prepost if necessary\n",
    "    filename = files[index]\n",
    "    if prepost.lower() == \"pre\":\n",
    "        filename = filename.replace(\"Post\", \"Pre\")\n",
    "    \n",
    "    return filename\n",
    "\n",
    "\n",
    "# Define the file path pattern\n",
    "# file_path_pattern = r\"C:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\B_avgScans_HC*MGHRA-.csv\"\n",
    "# file_path_pattern = r\"C:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\B_avgScans_HC0[1-8]MGHRA-.csv\"\n",
    "# file_path_pattern = r\"C:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\B_avgScans_HC01MGHRA-.csv\"\n",
    "# file_path_pattern = r\"C:\\Users\\omnic\\OneDrive\\Desktop\\HC MGRH preprocesssed\\pre_post\\B_avgScans_HD07FKCC-Post.csv\"\n",
    "\n",
    "file_path_pattern = get_filename_from_index(index, prepost)\n",
    "print(file_path_pattern)\n",
    "\n",
    "# Use glob to find all matching file paths\n",
    "file_paths = glob.glob(file_path_pattern)\n",
    "\n",
    "# List to store dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Load and process each file\n",
    "for file_path in file_paths:\n",
    "    # Read the file, skipping the first 40 rows\n",
    "    df = pd.read_csv(file_path, skiprows=39)\n",
    "    dataframes.append(df)\n",
    "    print(f\"Loaded file: {file_path}\")\n",
    "\n",
    "# Ensure all DataFrames have the same structure\n",
    "if not all(df.columns.equals(dataframes[0].columns) for df in dataframes):\n",
    "    raise ValueError(\"Files do not have matching columns.\")\n",
    "\n",
    "# print out the columns\n",
    "for df in dataframes:\n",
    "    print(dataframes[0].columns)\n",
    "    # apparently there's a value in df.values that has a non-numeric value\n",
    "    # let's find it\n",
    "    for i, row in enumerate(df.values):\n",
    "        for j, value in enumerate(row):\n",
    "            try:\n",
    "                float(value)\n",
    "            except:\n",
    "                print(f\"Non-numeric value at row {i} column {j}: {value}\")\n",
    "    # data = jnp.array(df.values)\n",
    "\n",
    "print(f\"signal length: {len(dataframes[0])}\")\n",
    "\n",
    "# Stack all DataFrames to calculate the mean\n",
    "stacked_data = jnp.stack([jnp.array(df.values) for df in dataframes])\n",
    "\n",
    "# the shape of this is (12, 2725, 2)\n",
    "# this is (signal-number, time_point, [time,amplitude])\n",
    "\n",
    "# normalize the data in each row via the average of their first three values\n",
    "normalization_factor = jnp.mean(stacked_data[:, :3, 1], axis=1)[:, None]\n",
    "stacked_data = stacked_data.at[:, :, 1].set(stacked_data[:, :, 1]/normalization_factor)\n",
    "\n",
    "# Calculate the average across the files\n",
    "average_data = jnp.mean(stacked_data, axis=0)\n",
    "time_data = average_data[:, 0]*1000\n",
    "noisy_signal = average_data[:, 1]\n",
    "\n",
    "# Plot the average data\n",
    "plt.plot(time_data, noisy_signal)\n",
    "plt.axhline(0, color=\"black\", linewidth=0.5)\n",
    "plt.xlabel(\"Time (ms)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.title(\"Average Data\")\n",
    "plt.show()\n",
    "\n",
    "# add the name of the file to copy data, between the last underscore and the dot\n",
    "copy_dict[\"Name\"] = file_path_pattern.split(\"_\")[-1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#################\n",
    "#### SOLVERS ####\n",
    "#################\n",
    "\n",
    "## for running solvers look to the 1 decay example\n",
    "\n",
    "## 1 DECAY\n",
    "\n",
    "# set magnitudes for de-dimensionalization\n",
    "magnitudes_1 = jnp.array([\n",
    "    1.0, 80.0,\n",
    "])\n",
    "\n",
    "# get the solver function\n",
    "solve_1 = get_solve(\n",
    "    maxiter=2000,\n",
    "    atol=1e-20,\n",
    "    time_data=time_data,\n",
    "    noisy_signal=noisy_signal,\n",
    "    magnitudes=magnitudes_1,\n",
    ")\n",
    "\n",
    "# set the decay constant starting values\n",
    "decay_constants_1 = jnp.array([\n",
    "    1.0, 50,\n",
    "])\n",
    "\n",
    "# run the solver\n",
    "params_1, state_1 = solve_1(decay_constants_1)\n",
    "\n",
    "# All this done will produce a jit compiled solve_1 function that can be called \n",
    "# with different starting positions and runs outrageously fast.\n",
    "\n",
    "## 2 DECAYS\n",
    "\n",
    "magnitudes_2 = jnp.array([\n",
    "    0.5, 40.0,\n",
    "    0.5, 160.0,\n",
    "])\n",
    "\n",
    "solve_2 = get_solve(\n",
    "    maxiter=2000,\n",
    "    atol=1e-20,\n",
    "    time_data=time_data,\n",
    "    noisy_signal=noisy_signal,\n",
    "    magnitudes=magnitudes_2,\n",
    ")\n",
    "\n",
    "param_list = []\n",
    "state_list = []\n",
    "scaling_factors = jnp.linspace(1, 2, 10)\n",
    "amplitude_ratios = jnp.linspace(2, 3, 10)\n",
    "\n",
    "# Sampling the parameter space to find the best fit\n",
    "for scale in tqdm(scaling_factors):\n",
    "    for ratio in amplitude_ratios:\n",
    "        # scale represents the scaling factor for the decay constants\n",
    "        # ratio represents the amplitude ratio between the two decays\n",
    "        # the two together allow us to sample all of the likely parts of \n",
    "        # the parameter space given the results of the first decay\n",
    "        \n",
    "        ratio_left = (ratio-1)/ratio\n",
    "        decay_constants_2 = jnp.array([\n",
    "            params_1[0]*ratio_left, params_1[1]/scale,\n",
    "            params_1[0]/ratio, params_1[1]*scale,\n",
    "        ])\n",
    "    \n",
    "        params_2, state_2 = solve_2(decay_constants_2)\n",
    "        param_list.append(params_2)\n",
    "        state_list.append(state_2)\n",
    "\n",
    "\n",
    "        decay_constants_2 = jnp.array([\n",
    "            params_1[0]/ratio, params_1[1]/scale,\n",
    "            params_1[0]*ratio_left, params_1[1]*scale,\n",
    "        ])\n",
    "    \n",
    "        params_2, state_2 = solve_2(decay_constants_2)\n",
    "        param_list.append(params_2)\n",
    "        state_list.append(state_2)\n",
    "\n",
    "# Select the best fit, excluding any with negative values, but produce a warning\n",
    "positive_found = False\n",
    "while not positive_found:\n",
    "    if len(state_list) == 0:\n",
    "        break\n",
    "    best_index = jnp.argmin(jnp.array([state.value for state in state_list]))\n",
    "    params_2 = param_list[best_index]\n",
    "    state_2 = state_list[best_index]\n",
    "    if jnp.all(params_2 > 0) and jnp.sum(params_2[:-1][::2]) < 1:\n",
    "        positive_found = True\n",
    "    else:\n",
    "        param_list.pop(best_index)\n",
    "        state_list.pop(best_index)\n",
    "        print(\"parameters rejected\")\n",
    "\n",
    "if positive_found:\n",
    "    print(\"No negative values found in parameters\")\n",
    "    params_2 = sort_params(params_2)\n",
    "    print_params(params_2)\n",
    "else:\n",
    "    print(\"All parameter sets had negative values\")\n",
    "\n",
    "## 3 DECAYS\n",
    "\n",
    "magnitudes_3 = jnp.array([\n",
    "    0.5, 20.0,\n",
    "    0.5, 50.0,\n",
    "    0.5, 200.0,\n",
    "])\n",
    "\n",
    "solve_3 = get_solve(\n",
    "    maxiter=2000,\n",
    "    atol=1e-20,\n",
    "    time_data=time_data,\n",
    "    noisy_signal=noisy_signal,\n",
    "    magnitudes=magnitudes_3,\n",
    ")\n",
    "\n",
    "param_list = []\n",
    "state_list = []\n",
    "scaling_factors = jnp.linspace(1, 1.5, 10)\n",
    "amplitude_ratios = jnp.linspace(2, 3.5, 40)\n",
    "\n",
    "for scale in tqdm(scaling_factors):\n",
    "    for ratio in amplitude_ratios:\n",
    "        # Very similar scaling to the 2 decay case\n",
    "    \n",
    "        ratio_left = (ratio-1)/ratio\n",
    "        \n",
    "        decay_constants_3 = jnp.array([\n",
    "            params_2[0]*ratio_left,             # A1 = A1_prev * (ratio-1)/ratio\n",
    "            params_2[1]/scale,                  # tau1 = tau1_prev/scale\n",
    "                \n",
    "            (params_2[0] + params_2[2])/ratio,  # A2 = (A1_prev + A2_prev)/ratio\n",
    "            (params_2[1]+params_2[3])/2,        # tau2 = (tau1_prev + tau2_prev)/2\n",
    "                \n",
    "            params_2[2]*ratio_left,             # A3 = A2_prev * (ratio-1)/ratio\n",
    "            params_2[3]*scale,                  # tau3 = tau2_prev*scale\n",
    "        ])\n",
    "        \n",
    "        params_3, state_3 = solve_3(decay_constants_3)\n",
    "        \n",
    "        param_list.append(params_3)\n",
    "        state_list.append(state_3)\n",
    "\n",
    "# Second section to identify fits relating to early decays\n",
    "amplitudes = jnp.linspace(0.001, 0.5, 10)\n",
    "decay_constants = jnp.linspace(1, 20, 20)\n",
    "for tau0 in decay_constants:\n",
    "    for a0 in amplitudes:\n",
    "        \n",
    "        decay_constants_3 = jnp.array([\n",
    "                a0,             # A1 = A1_prev * (ratio-1)/ratio\n",
    "                tau0,                  # tau1 = tau1_prev/scale\n",
    "                    \n",
    "                params_2[0],  # A2 = (A1_prev + A2_prev)/ratio\n",
    "                params_2[1],        # tau2 = (tau1_prev + tau2_prev)/2\n",
    "                    \n",
    "                params_2[2],             # A3 = A2_prev * (ratio-1)/ratio\n",
    "                params_2[3],                  # tau3 = tau2_prev*scale\n",
    "            ])\n",
    "\n",
    "        params_3, state_3 = solve_3(decay_constants_3)\n",
    "        \n",
    "        param_list.append(params_3)\n",
    "        state_list.append(state_3)\n",
    "\n",
    "# Select the best fit, excluding any with negative values, but produce a warning\n",
    "positive_found = False\n",
    "while not positive_found:\n",
    "    best_index = jnp.argmin(jnp.array([state.value for state in state_list]))\n",
    "    params_3 = param_list[best_index]\n",
    "    state_3 = state_list[best_index]\n",
    "    if jnp.all(params_3 > 0):\n",
    "        positive_found = True\n",
    "    else:\n",
    "        print(\"Negative values found in parameters:\")\n",
    "        print_params(params_3)\n",
    "        param_list.pop(best_index)\n",
    "        state_list.pop(best_index)\n",
    "\n",
    "if positive_found:\n",
    "    print(\"No negative values found in parameters\")\n",
    "    params_3 = sort_params(params_3)\n",
    "    print_params(params_3)\n",
    "else:\n",
    "    print(\"All parameter sets had negative values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Guide to understanding statistics:\")\n",
    "print(\"Autocorrelation: 0 indicates no correlation, 1 indicates a perfect correlation. This only compares each \\n\\tpoint to the next, so it is not a perfect measure of correlation.\")\n",
    "print(\"Shapiro-Wilk: p value > 0.05 typically indicates normality, however, this evaluates the distribution as \\n\\ta whole and does not utilise signal structure or any temporal information.\")\n",
    "\n",
    "# Print the best fits for each\n",
    "for i in range(1, 4):\n",
    "    params = locals()[f\"params_{i}\"]\n",
    "    state = locals()[f\"state_{i}\"]\n",
    "    \n",
    "    # Print the final parameters and the loss\n",
    "    print(f\"\\n{i} decay{'' if i==1 else 's'} with loss of {state.value:.4f}: \")\n",
    "    print_params(params)\n",
    "    fit_residual = noisy_signal - fit_multi_exponential(time_data, *params)\n",
    "    \n",
    "    # calcualte the fit SNR\n",
    "    fit_SNR = jnp.sum(params[::2])/jnp.std(fit_residual)\n",
    "    \n",
    "    # Print the statistics\n",
    "    dw = durbin_watson(fit_residual)\n",
    "    print(f\"Autocorrelation: {dw:.4f}\")\n",
    "    sw = shapiro_wilk(fit_residual)\n",
    "    print(f\"Shapiro-Wilk statistic: {sw[0]:.5f}, p value: {sw[1]:.5f}\")\n",
    "    if i>1:\n",
    "        # Print the relative amplitudes\n",
    "        print(f\"Relative amplitudes:\")\n",
    "        amplitudes = params[::2]\n",
    "        amp_sum = jnp.sum(amplitudes[params[1::2] > 20])\n",
    "        amplitudes = amplitudes/amp_sum\n",
    "        print(\"[\" + \", \".join([f\"{amp:.6f}\" for amp in amplitudes]) + \"]\")\n",
    "    \n",
    "    # Add parameters to the copy dict    \n",
    "    if i == 2:\n",
    "        # Add SNR to the copy dict\n",
    "        copy_dict[\"SNR_2\"] = fit_SNR\n",
    "        # Add Tau1, Tau2, RA2 to the copy dict\n",
    "        copy_dict[\"Tau1_2\"] = params[1]\n",
    "        copy_dict[\"Tau2_2\"] = params[3]\n",
    "        copy_dict[\"RA2_2\"] = amplitudes[1]\n",
    "        # Add statistics to the copy dict\n",
    "        copy_dict[\"swp_2\"] = sw[1]\n",
    "        copy_dict[\"dw_2\"] = dw\n",
    "    elif i == 3:\n",
    "        # Add SNR to the copy dict\n",
    "        copy_dict[\"SNR_3\"] = fit_SNR\n",
    "        # Add all taus and RAs to the copy dict\n",
    "        copy_dict[\"Tau1_3\"] = params[1]\n",
    "        copy_dict[\"Tau2_3\"] = params[3]\n",
    "        copy_dict[\"Tau3_3\"] = params[5]\n",
    "        copy_dict[\"RA1_3\"] = amplitudes[0]\n",
    "        copy_dict[\"RA2_3\"] = amplitudes[1]\n",
    "        copy_dict[\"RA3_3\"] = amplitudes[2]\n",
    "        # Add statistics to the copy dict\n",
    "        copy_dict[\"swp_3\"] = sw[1]\n",
    "        copy_dict[\"dw_3\"] = dw \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################\n",
    "#### PLOT FITS & RESIDUALS ####\n",
    "###############################\n",
    "\n",
    "\n",
    "def low_pass_filter(signal, cutoff_freq, sample_rate):\n",
    "    # Compute the Fourier transform of the signal\n",
    "    signal_freq = fft(signal)\n",
    "    \n",
    "    # Create the frequency domain\n",
    "    n = signal_freq.shape[0]\n",
    "    freqs = jnp.fft.fftfreq(n, d=1/sample_rate)\n",
    "    \n",
    "    # Create the filter\n",
    "    filter = jnp.where(jnp.abs(freqs) < cutoff_freq, 1.0, 0.0)\n",
    "    \n",
    "    # Apply the filter\n",
    "    filtered_signal_freq = signal_freq * filter\n",
    "    \n",
    "    # Compute the inverse Fourier transform\n",
    "    filtered_signal = jnp.real(ifft(filtered_signal_freq))\n",
    "    \n",
    "    return filtered_signal\n",
    "\n",
    "# Plot a comparions of the three\n",
    "for i in range(2):\n",
    "    plt.figure()\n",
    "    plt.plot(time_data, noisy_signal, label=\"Data\")\n",
    "    plt.plot(time_data, fit_multi_exponential(time_data, *params_1), label=\"1 Decay\")\n",
    "    plt.plot(time_data, fit_multi_exponential(time_data, *params_2), label=\"2 Decays\")\n",
    "    plt.plot(time_data, fit_multi_exponential(time_data, *params_3), label=\"3 Decays\")\n",
    "    plt.xlabel(\"Time (ms)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.title(\"Fits\")\n",
    "    if i == 1:\n",
    "        plt.xscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the residuals\n",
    "sample_rate = 1/(time_data[1] - time_data[0])\n",
    "cutoff_freq = 0.05\n",
    "for i in range(1, 4):\n",
    "    \n",
    "    params = locals()[f\"params_{i}\"]\n",
    "    residuals = noisy_signal - fit_multi_exponential(time_data, *params)\n",
    "    low_pass_residuals = low_pass_filter(residuals, cutoff_freq, sample_rate)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(time_data, residuals, label=f\"{i} Decays\")\n",
    "    plt.plot(time_data, low_pass_residuals, label=f\"{i} Decays Low Pass\")\n",
    "    plt.axhline(0, color=\"black\", linewidth=0.5)\n",
    "    plt.xlabel(\"Time (ms)\")\n",
    "    plt.ylabel(\"Residual amplitude\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################\n",
    "#### BOOTSTRAP CONFIDENCE INTERVALS ####\n",
    "########################################\n",
    "\n",
    "# Define hyperparameters\n",
    "# number of bootstraps to run\n",
    "n_bootstrap = 1000\n",
    "# keys for random number generation\n",
    "rng, key = random.split(random.PRNGKey(2025), 2)\n",
    "\n",
    "for dec_index in range(2, 4):\n",
    "    params = locals()[f\"params_{dec_index}\"]\n",
    "    print(f\"\\n{dec_index} decays:\")\n",
    "    \n",
    "    # Run the bootstrap\n",
    "    bootstrap_params = jnp.zeros((n_bootstrap, len(params)))\n",
    "\n",
    "    # Define the bootstrap solver\n",
    "    bootstrap_solve = get_bootstrap_solve(\n",
    "        atol=1e-20,\n",
    "        time_data=time_data,\n",
    "        noisy_signal=noisy_signal,\n",
    "        params=params,\n",
    "    )\n",
    "\n",
    "    start = time()\n",
    "    for i in tqdm(range(n_bootstrap)):\n",
    "        rng, key = random.split(rng)\n",
    "        bootstrap_params = bootstrap_params.at[i].set(bootstrap_solve(key)[0])\n",
    "    print(f\"{i+1} bootstraps complete in {time()-start:.2f} seconds.\")\n",
    "\n",
    "    print(f\"shape of bootstrap_params before exclusion: \", bootstrap_params.shape)\n",
    "    # first exclude non-physical parameters\n",
    "    for i in range(len(params)):\n",
    "        bootstrap_params = bootstrap_params[bootstrap_params[:, i] > 0]\n",
    "    for i in range(0, len(params), 2):\n",
    "        bootstrap_params = bootstrap_params[bootstrap_params[:, i+1] < time_data[-1]]\n",
    "    print(f\"shape of bootstrap_params after exclusion: \", bootstrap_params.shape)\n",
    "    \n",
    "    # Perform normalization of amplitudes\n",
    "    contains_low_tau = jnp.sum(params[1::2] < 20)\n",
    "    for i in range(n_bootstrap):\n",
    "        \n",
    "        amplitudes = bootstrap_params[i, ::2]\n",
    "        taus = bootstrap_params[i, 1::2]\n",
    "        amp_sum = jnp.sum(amplitudes[taus > 20])\n",
    "        bootstrap_params = bootstrap_params.at[i, ::2].set(bootstrap_params[i, ::2] / amp_sum)\n",
    "\n",
    "    # plot the bootstrap parameters\n",
    "    fig, ax = plt.subplots(params.shape[0]//2, 2)\n",
    "    ax = ax.flatten()\n",
    "    for j in range(params.shape[0]):\n",
    "        ax[j].hist(bootstrap_params[:, j], bins=200)\n",
    "        ax[j].set_title(f\"Parameter {j}\")\n",
    "    plt.suptitle(\"Bootstrap Parameter Distributions\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # now to find the 95% confidence intervals\n",
    "    contains_low_tau = jnp.sum(params[1::2] < 20)\n",
    "    for i in range(params.shape[0]):\n",
    "        lower = jnp.percentile(bootstrap_params[:, i], 2.5)\n",
    "        upper = jnp.percentile(bootstrap_params[:, i], 97.5)\n",
    "        \n",
    "        parameter_name = (\"RA\" if i%2 == 0 else \"Tau\") + str(i//2 + 1 - contains_low_tau)\n",
    "        print(f\"{parameter_name}: \\t{lower:.6f} to {upper:.6f}\")\n",
    "        \n",
    "        # add values to the copy dict\n",
    "        if dec_index == 2 and i%2 == 0 and i//2 + 1 == 2:\n",
    "            copy_dict[\"RA2_2_lower\"] = lower\n",
    "            copy_dict[\"RA2_2_upper\"] = upper\n",
    "        elif dec_index == 3 and i%2 == 0:\n",
    "            if i//2 + 1 == 2:\n",
    "                copy_dict[\"RA2_3_lower\"] = lower\n",
    "                copy_dict[\"RA2_3_upper\"] = upper\n",
    "            elif i//2 + 1 == 3:\n",
    "                copy_dict[\"RA3_3_lower\"] = lower\n",
    "                copy_dict[\"RA3_3_upper\"] = upper\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to format the copy_dict into a string that can be copied and pasted into google sheets\n",
    "\n",
    "# The order of the string is important, so we will define the order here\n",
    "string_order = [\n",
    "    \"Name\",\n",
    "    \n",
    "    \"SNR_2\", \"Tau1_2\", \"Tau2_2\", \"RA2_2\", \n",
    "    \"RA2_2_lower\", \"RA2_2_upper\", \n",
    "    \"swp_2\", \"dw_2\",\n",
    "    \n",
    "    \"SNR_3\", \"Tau1_3\", \"Tau2_3\", \"Tau3_3\", \"RA1_3\", \"RA2_3\", \"RA3_3\",\n",
    "    \"RA2_3_lower\", \"RA2_3_upper\", \"RA3_3_lower\", \"RA3_3_upper\",\n",
    "    \"swp_3\", \"dw_3\",\n",
    "]\n",
    "\n",
    "# taus will be to 2 decimal places\n",
    "# RA values and their lower and upper confidence intervals will be to 6 decimal places\n",
    "# swp and dw will be to 5 decimal places\n",
    "# snr will be to 2 decimal place\n",
    "\n",
    "from jaxlib.xla_extension import ArrayImpl\n",
    "\n",
    "copy_string = \"\"\n",
    "\n",
    "for key in string_order:\n",
    "    if key in copy_dict:\n",
    "        if type(copy_dict[key]) == ArrayImpl:\n",
    "            # print(\"We got one!!!!\")\n",
    "            # copy_dict[key] = copy_dict[key].astype(copy_dict[key].dtype)\n",
    "            copy_dict[key] = copy_dict[key].tolist()\n",
    "\n",
    "print(copy_dict)\n",
    "\n",
    "# now to format the string\n",
    "for key in string_order:\n",
    "    if key not in copy_dict:\n",
    "        copy_string += \", \"\n",
    "        print(f\"WARNING: {key} not found in copy_dict\")\n",
    "        continue\n",
    "    # get the value\n",
    "    value = copy_dict[key]\n",
    "    \n",
    "    if type(value) == str:\n",
    "        copy_string += value + \", \"\n",
    "    elif type(value) in [float, jnp.float32, jnp.float64]:\n",
    "        \n",
    "        # format the value based on the specification above\n",
    "        if \"RA\" in key:\n",
    "            copy_string += f\"{value:.6f}, \"\n",
    "        elif \"Tau\" in key:\n",
    "            copy_string += f\"{value:.2f}, \"\n",
    "        elif \"swp\" in key or \"dw\" in key:\n",
    "            copy_string += f\"{value:.5f}, \"\n",
    "        elif \"SNR\" in key:\n",
    "            copy_string += f\"{value:.2f}, \"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid type for value: \" + str(type(value)))\n",
    "\n",
    "# remove the last comma and space becasue we're civilized\n",
    "copy_string = copy_string[:-2]\n",
    "\n",
    "# print the string and make sure not to copy the name\n",
    "print(copy_string)\n",
    "print(f\"\\nsignal length: {len(noisy_signal)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
